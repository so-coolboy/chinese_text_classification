{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext用于中文文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText论文中提到了一些tricks\n",
    "\n",
    "- hierarchical softmax\n",
    "    - 类别数较多时，通过构建一个霍夫曼编码树来加速softmax layer的计算，和之前word2vec中的trick相同\n",
    "- N-gram features\n",
    "    - 只用unigram的话会丢掉word order信息，所以通过加入N-gram features进行补充用hashing来减少N-gram的存储\n",
    "- Subword\n",
    "    - 对一些出现次数很少或者没有出现的词，使用subword的词向量之和来表达，如coresponse这个词，使用co的词向量与response的词向量之和来表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText做文本分类要求文本是如下的存储形式：\n",
    "```\n",
    "__label__2 , birchas chaim , yeshiva birchas chaim is a orthodox jewish mesivta high school in lakewood township new jersey . it was founded by rabbi shmuel zalmen stein in 2001 after his father rabbi chaim stein asked him to open a branch of telshe yeshiva in lakewood . as of the 2009-10 school year the school had an enrollment of 76 students and 6 . 6 classroom teachers ( on a fte basis ) for a student–teacher ratio of 11 . 5 1 . \n",
    "__label__6 , motor torpedo boat pt-41 , motor torpedo boat pt-41 was a pt-20-class motor torpedo boat of the united states navy built by the electric launch company of bayonne new jersey . the boat was laid down as motor boat submarine chaser ptc-21 but was reclassified as pt-41 prior to its launch on 8 july 1941 and was completed on 23 july 1941 . \n",
    "__label__11 , passiflora picturata , passiflora picturata is a species of passion flower in the passifloraceae family . \n",
    "__label__13 , naya din nai raat , naya din nai raat is a 1974 bollywood drama film directed by a . bhimsingh . the film is famous as sanjeev kumar reprised the nine-role epic performance by sivaji ganesan in navarathri ( 1964 ) which was also previously reprised by akkineni nageswara rao in navarathri ( telugu 1966 ) . this film had enhanced his status and reputation as an actor in hindi cinema . \n",
    "```\n",
    "其中前面的`__label__`是前缀，也可以自己定义，`__label__`后接的为类别。\n",
    "\n",
    "我们定义我们的5个类别分别为：\n",
    "```\n",
    "1:technology\n",
    "2:car\n",
    "3:entertainment\n",
    "4:military\n",
    "5:sports\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 设定各类类别映射，如'technology'为1，'car'为2……\n",
    "cate_dic = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "# 读取数据\n",
    "df_technology = pd.read_csv(\"./origin_data/technology_news.csv\", encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"./origin_data/car_news.csv\", encoding='utf-8')\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"./origin_data/entertainment_news.csv\", encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"./origin_data/military_news.csv\", encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"./origin_data/sports_news.csv\", encoding='utf-8')\n",
    "df_sports = df_sports.dropna()\n",
    "# 转换为list列表的形式\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[:20000]\n",
    "military = df_military.content.values.tolist()[:20000]\n",
    "sports = df_sports.content.values.tolist()[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入停用词表 并定义本文处理函数，将文本处理为fasttext的输入格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"origin_data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stopwords=stopwords['stopword'].values\n",
    "\n",
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs=jieba.lcut(line)\n",
    "            # 去标点、停用词等\n",
    "            segs = list(filter(lambda x:len(x)>1, segs))\n",
    "            segs = list(filter(lambda x:x not in stopwords, segs))\n",
    "            # 将句子处理成  __label__1 词语 词语 词语 ……的形式\n",
    "            sentences.append(\"__label__\"+str(category)+\" , \"+\" \".join(segs))\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.742 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#生成训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text(technology, sentences, cate_dic['technology'])\n",
    "preprocess_text(car, sentences, cate_dic['car'])\n",
    "preprocess_text(entertainment, sentences, cate_dic['entertainment'])\n",
    "preprocess_text(military, sentences, cate_dic['military'])\n",
    "preprocess_text(sports, sentences, cate_dic['sports'])\n",
    "\n",
    "# 随机打乱数据\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to fasttext format...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# 将数据保存到train_data.txt中\n",
    "print(\"writing data to fasttext format...\")\n",
    "out = open('train_data.txt', 'w', encoding='utf-8')\n",
    "for sentence in sentences:\n",
    "    out.write(sentence+\"\\n\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用fastText训练生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\"\"\"\n",
    "  训练一个监督模型, 返回一个模型对象\n",
    "\n",
    "  @param input:           训练数据文件路径\n",
    "  @param lr:              学习率\n",
    "  @param dim:             向量维度\n",
    "  @param ws:              cbow模型时使用\n",
    "  @param epoch:           次数\n",
    "  @param minCount:        词频阈值, 小于该值在初始化时会过滤掉\n",
    "  @param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉\n",
    "  @param minn:            构造subword时最小char个数\n",
    "  @param maxn:            构造subword时最大char个数\n",
    "  @param neg:             负采样\n",
    "  @param wordNgrams:      n-gram个数\n",
    "  @param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax\n",
    "  @param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量\n",
    "  @param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出\n",
    "  @param lrUpdateRate:    学习率更新\n",
    "  @param t:               负采样阈值\n",
    "  @param label:           类别前缀\n",
    "  @param verbose:         ??\n",
    "  @param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机\n",
    "  @return model object\n",
    "\"\"\"\n",
    "classifier = fasttext.train_supervised(input='train_data.txt', dim=100, epoch=5,\n",
    "                                         lr=0.1, wordNgrams=2, loss='softmax')\n",
    "classifier.save_model('classifier.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对模型效果进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.9838656268198271\n",
      "R@1: 0.9838656268198271\n",
      "Number of examples: 87577\n"
     ]
    }
   ],
   "source": [
    "result = classifier.test('train_data.txt')\n",
    "print('P@1:', result[1])\n",
    "print('R@1:', result[2])\n",
    "print('Number of examples:', result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "### 实际预测\n",
    "label_to_cate = {'__label__1':'technology', '__label__2':'car', '__label__3':'entertainment',\n",
    "                 '__label__4':'military', '__label__5':'sports'}\n",
    "\n",
    "texts = '这 是 中国 制造 宝马 汽车'\n",
    "labels = classifier.predict(texts)\n",
    "# print(labels)\n",
    "print(label_to_cate[labels[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K 个预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测：car\t概率为： 1.000009\n",
      "预测：technology\t概率为： 0.000011\n",
      "预测：military\t概率为： 0.000010\n"
     ]
    }
   ],
   "source": [
    "labels = classifier.predict(texts, k=3)\n",
    "label, proba = labels[0], labels[1]\n",
    "for label, proba in zip(label, proba):\n",
    "    print('预测：%s\\t概率为： %f' % (label_to_cate[label], proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext用于中文无监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to fasttext unsupervised learning format...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_unsupervised(content_lines, sentences):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs=jieba.lcut(line)\n",
    "            segs = list(filter(lambda x:len(x)>1, segs))\n",
    "            segs = list(filter(lambda x:x not in stopwords, segs))\n",
    "            # 处理成  词语 词语 词语…… 的形式\n",
    "            sentences.append(\" \".join(segs))\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            continue\n",
    "#生成无监督训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text_unsupervised(technology, sentences)\n",
    "preprocess_text_unsupervised(car, sentences)\n",
    "preprocess_text_unsupervised(entertainment, sentences)\n",
    "preprocess_text_unsupervised(military, sentences)\n",
    "preprocess_text_unsupervised(sports, sentences)\n",
    "\n",
    "print(\"writing data to fasttext unsupervised learning format...\")\n",
    "out = open('unsupervised_train_data.txt', 'w', encoding='utf-8')\n",
    "for sentence in sentences:\n",
    "    out.write(sentence+\"\\n\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '中国', '发展', '汽车', '用户', '技术', '比赛', '市场', '平台', '服务']\n",
      "['</s>', '中国', '发展', '汽车', '用户', '技术', '比赛', '市场', '平台', '服务']\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Skipgram model\n",
    "model = fasttext.train_unsupervised('unsupervised_train_data.txt', model='skipgram')\n",
    "print(model.words[:10]) # list of words in dictionary\n",
    "\n",
    "# CBOW model\n",
    "model = fasttext.train_unsupervised('unsupervised_train_data.txt', model='cbow')\n",
    "print(model.words[:10]) # list of words in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.3030976   0.9404861  -0.01847073 -2.0281935  -0.47557053  1.4022453\n",
      " -0.33904338  1.0796572   0.02798706  2.1330366  -0.18552518 -0.2461389\n",
      " -0.96020913 -0.60029024 -0.3148932   2.2439585  -2.0460677  -2.2157037\n",
      " -0.5819198  -0.0692748   0.26359314 -0.29606423  1.8785787  -0.19154015\n",
      " -1.3072726  -0.06210047  0.74192524 -0.5015831  -0.9866113  -0.5674383\n",
      " -0.9844613  -0.50053316  1.5576434  -0.1627377  -2.2799628  -0.83161664\n",
      " -3.1632657  -0.15478554 -1.1918309   1.7669501   0.8818059   0.78309166\n",
      "  0.7428605   0.01461019  0.9616978  -2.0978618   1.9600568  -0.9531319\n",
      "  0.35986143  1.4861448  -2.2054806   1.4554088   0.1940116   0.91389835\n",
      " -0.06472382 -1.0512189   0.95620656 -0.8704989  -2.5449433  -1.335377\n",
      "  0.5264219  -2.4620938   2.6068513  -0.10895383  1.7347517   0.9680276\n",
      " -0.57421255  3.0573      0.07793453 -0.37695345  0.75320894 -1.8995788\n",
      " -1.1326122  -0.45068133  0.58303857 -0.06479045  1.364764   -0.8579126\n",
      "  0.9971492   0.5678582   0.84928197 -1.400441    1.0710597  -1.5964117\n",
      "  0.10242309 -0.91553605 -0.32349306  0.11777124  1.3702997   1.4825792\n",
      " -0.34443256 -1.1412066  -0.42505848 -2.5061321  -0.01928405  0.01531881\n",
      " -1.0386747   0.330808    0.8082226  -1.2001617 ]\n"
     ]
    }
   ],
   "source": [
    "# 查看某个词的词向量\n",
    "print(model['赛季'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比gensim的word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.728 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_unsupervised(content_lines, sentences):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs=jieba.lcut(line)\n",
    "            segs = list(filter(lambda x:len(x)>1, segs))\n",
    "            segs = list(filter(lambda x:x not in stopwords, segs))\n",
    "            # gensim 输入格式为 [词， 词， 词]\n",
    "            sentences.append(segs)\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            continue\n",
    "#生成无监督训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text_unsupervised(technology, sentences)\n",
    "preprocess_text_unsupervised(car, sentences)\n",
    "preprocess_text_unsupervised(entertainment, sentences)\n",
    "preprocess_text_unsupervised(military, sentences)\n",
    "preprocess_text_unsupervised(sports, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "model.save(\"gensim_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.8249986   0.03112162 -0.25813046  0.33364102 -1.6184547  -1.8084553\n",
      "  2.1597168   0.1351388  -0.53724444  0.33972478  2.1969304  -0.14570096\n",
      " -0.7215449   0.12304255 -0.19669688  1.0330333   0.55557615  1.9715163\n",
      " -0.62517285 -1.4554526  -2.1550758  -0.75689536  0.7878873  -1.8041505\n",
      "  0.6196159   0.16723332 -0.82358366 -0.27559796 -0.7487638   1.9861195\n",
      " -0.8994759  -0.9230798  -1.3786101   0.41648138 -1.6454602   0.3242791\n",
      " -1.3978794   0.08787971  0.8411618   1.4878358  -0.14459854  1.2883182\n",
      "  1.4306669  -2.1317682   0.15039028  1.1206025   0.13805757 -1.6349252\n",
      " -0.9597154   1.2816765   0.6698505   0.05626296 -1.1199676  -1.0730348\n",
      " -0.79737777  1.8237026  -2.020744    0.91954416  1.2784522  -1.3709328\n",
      "  0.8549252  -0.33926338 -1.3959678   0.2701675  -1.1709216  -0.83576757\n",
      " -0.8271942   1.1551841  -1.6863061   0.05112632 -0.1284147   0.33976018\n",
      "  0.7828476  -1.5634745  -0.8728223  -1.9741758  -0.5336166  -1.6338608\n",
      " -1.2736895  -1.3195976  -0.44755006 -0.57404816  0.54282814  0.8669889\n",
      " -1.7347049  -1.3960758   0.88830245  0.8141533  -0.47331348  0.1727305\n",
      "  0.6829921  -1.8338073  -1.2978863  -0.48163927 -0.9774465   1.1009849\n",
      " -0.9605543   0.6604892  -1.6103339   0.49157014]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['赛季'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('亚冠', 0.8779898881912231), ('中甲', 0.8449328541755676), ('BIG4', 0.8394372463226318), ('本赛季', 0.8369283676147461), ('辽足', 0.8342424631118774), ('国安', 0.8320003747940063), ('恒大', 0.8271181583404541), ('名额', 0.825170636177063), ('全北', 0.8210830688476562), ('强赛', 0.8180699348449707)]\n"
     ]
    }
   ],
   "source": [
    "# 寻找相似词语\n",
    "print(model.wv.most_similar('赛季'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
